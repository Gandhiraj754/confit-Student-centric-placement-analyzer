{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-Cw01zzNTtq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUETVTZYz2MA"
      },
      "outputs": [],
      "source": [
        "!pip install streamlit pyngrok transformers sentencepiece accelerate graphviz rich --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R2A-UUe5U05"
      },
      "outputs": [],
      "source": [
        "!pip install pypdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.NGROK AUTH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2wbfV3E0HhH"
      },
      "outputs": [],
      "source": [
        "NGROK_AUTH_TOKEN = \"ADD_YOUR_NGROK_TOKEN\"\n",
        "\n",
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4.APP.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iozZz4v090pZ"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from graphviz import Digraph\n",
        "from pypdf import PdfReader\n",
        "import requests\n",
        "import json\n",
        "import os\n",
        "\n",
        "GPT_ENDPOINT = \"AZURE_GPT_ENDPOINT\"\n",
        "GPT_API_KEY = \"API_KEY\"\n",
        "EXTRACTION_MODEL = \"gpt-4o-mini\"\n",
        "STAGE4_MODEL_PATH = \"LOAD_MODEL_FROM_DRIVE\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class Stage4Model(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(\"intfloat/e5-base\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"intfloat/e5-base\")\n",
        "\n",
        "    def forward(self, text):\n",
        "        t = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "        out = self.encoder(**t).last_hidden_state[:, 0]\n",
        "        return torch.nn.functional.normalize(out, dim=-1)\n",
        "\n",
        "model = Stage4Model().to(device)\n",
        "\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    reader = PdfReader(pdf_file)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        try:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "        except:\n",
        "            pass\n",
        "    return text.strip()\n",
        "\n",
        "def azure_gpt_extract(text, mode=\"resume\"):\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"api-key\": GPT_API_KEY\n",
        "    }\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are an advanced ATS system. Extract structured information from this {mode} text.\n",
        "\n",
        "TEXT:\n",
        "{text}\n",
        "\n",
        "Return ONLY valid JSON:\n",
        "{{\n",
        "  \"skills\": [\"skill1\", \"skill2\"],\n",
        "  \"experience_years\": 0,\n",
        "  \"industry\": \"IT\"\n",
        "}}\n",
        "\n",
        "RULES:\n",
        "- ALWAYS extract at least 10 technical skills.\n",
        "- Experience must be inferred if not explicitly stated.\n",
        "- Industry must be short (IT, AI, HR, Finance).\n",
        "- No explanations. Only VALID JSON.\n",
        "\"\"\"\n",
        "\n",
        "    payload = {\n",
        "        \"model\": EXTRACTION_MODEL,\n",
        "        \"max_tokens\": 1024,\n",
        "        \"temperature\": 0.0,\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "    }\n",
        "\n",
        "    response = requests.post(GPT_ENDPOINT, headers=headers, json=payload).json()\n",
        "\n",
        "    try:\n",
        "        parsed = json.loads(response[\"choices\"][0][\"message\"][\"content\"])\n",
        "    except:\n",
        "        parsed = {\"skills\": [], \"experience_years\": 0, \"industry\": \"\"}\n",
        "\n",
        "    if len(parsed[\"skills\"]) < 5:\n",
        "\n",
        "        fallback_prompt = f\"\"\"\n",
        "Extract ONLY a JSON list of skills from this text.\n",
        "\n",
        "TEXT:\n",
        "{text}\n",
        "\n",
        "Return ONLY:\n",
        "{{\n",
        "  \"skills\": [\"skill1\", \"skill2\"]\n",
        "}}\n",
        "\"\"\"\n",
        "        fallback_payload = {\n",
        "            \"model\": EXTRACTION_MODEL,\n",
        "            \"max_tokens\": 512,\n",
        "            \"temperature\": 0.0,\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": fallback_prompt}]\n",
        "        }\n",
        "\n",
        "        fallback_resp = requests.post(GPT_ENDPOINT, headers=headers, json=fallback_payload).json()\n",
        "\n",
        "        try:\n",
        "            parsed[\"skills\"] = json.loads(fallback_resp[\"choices\"][0][\"message\"][\"content\"])[\"skills\"]\n",
        "        except:\n",
        "            parsed[\"skills\"] = []\n",
        "\n",
        "    parsed[\"skills\"] = sorted(list(set([s.lower().strip() for s in parsed[\"skills\"] if len(s) > 1])))\n",
        "\n",
        "    return parsed\n",
        "\n",
        "def compute_similarity(resume, job):\n",
        "    r = model.forward(resume).detach().cpu()\n",
        "    j = model.forward(job).detach().cpu()\n",
        "    return float(torch.nn.functional.cosine_similarity(r, j))\n",
        "\n",
        "def find_missing(resume_sk, job_sk):\n",
        "    r = [s.lower() for s in resume_sk]\n",
        "    return [s for s in job_sk if s.lower() not in r]\n",
        "\n",
        "def generate_summary(match, sim, missing, resume_sk, job_sk):\n",
        "    prompt = f\"\"\"\n",
        "Resume–JD Matching Summary\n",
        "\n",
        "Match Score: {match}%\n",
        "Similarity Score: {sim}\n",
        "Missing Skills: {missing}\n",
        "Resume Skills: {resume_sk}\n",
        "Job Skills: {job_sk}\n",
        "\n",
        "Write a short and professional summary of the candidate's fit.\n",
        "\"\"\"\n",
        "\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"api-key\": GPT_API_KEY\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": EXTRACTION_MODEL,\n",
        "        \"max_tokens\": 300,\n",
        "        \"temperature\": 0.2,\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
        "    }\n",
        "\n",
        "    response = requests.post(GPT_ENDPOINT, headers=headers, json=payload).json()\n",
        "\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
        "\n",
        "def plot_graph(resume_sk, job_sk, missing):\n",
        "    g = Digraph(\"SkillGraph\", format=\"png\")\n",
        "    g.attr(rankdir=\"LR\")\n",
        "\n",
        "    g.node(\"R\", \"Resume Skills\", shape=\"box\", fillcolor=\"#C8FFD4\", style=\"filled\")\n",
        "    g.node(\"J\", \"Job Skills\", shape=\"box\", fillcolor=\"#FFE0B5\", style=\"filled\")\n",
        "\n",
        "    for s in resume_sk:\n",
        "        g.node(f\"R_{s}\", s, color=\"green\")\n",
        "\n",
        "    for s in job_sk:\n",
        "        color = \"red\" if s.lower() in missing else \"green\"\n",
        "        g.node(f\"J_{s}\", s, color=color)\n",
        "\n",
        "    for s in job_sk:\n",
        "        if s.lower() in resume_sk:\n",
        "            g.edge(f\"R_{s}\", f\"J_{s}\", color=\"green\", label=\"match\")\n",
        "        else:\n",
        "            g.edge(\"R\", f\"J_{s}\", color=\"red\", label=\"missing\")\n",
        "\n",
        "    g.render(\"skill_graph\", cleanup=True)\n",
        "    return \"skill_graph.png\"\n",
        "\n",
        "st.title(\"Resume–JD AI Matcher (CONFIT + Azure GPT)\")\n",
        "\n",
        "resume_pdf = st.file_uploader(\"Upload Resume (PDF)\", type=[\"pdf\"])\n",
        "jd_pdf = st.file_uploader(\"Upload Job Description (PDF)\", type=[\"pdf\"])\n",
        "\n",
        "if st.button(\"Analyze Match\"):\n",
        "    if not resume_pdf or not jd_pdf:\n",
        "        st.error(\"Please upload both PDF files!\")\n",
        "        st.stop()\n",
        "\n",
        "    resume_text = extract_text_from_pdf(resume_pdf)\n",
        "    jd_text = extract_text_from_pdf(jd_pdf)\n",
        "\n",
        "    st.success(\"Text extracted from PDFs!\")\n",
        "\n",
        "    st.subheader(\"Resume Fields\")\n",
        "    resume_data = azure_gpt_extract(resume_text, \"resume\")\n",
        "    st.json(resume_data)\n",
        "\n",
        "    st.subheader(\"Job Fields\")\n",
        "    jd_data = azure_gpt_extract(jd_text, \"job\")\n",
        "    st.json(jd_data)\n",
        "\n",
        "    st.subheader(\"Match Score\")\n",
        "    sim = compute_similarity(resume_text, jd_text)\n",
        "    match = round(((sim + 1) / 2) * 100, 2)\n",
        "\n",
        "    st.metric(\"Similarity Score\", round(sim, 4))\n",
        "\n",
        "    st.subheader(\"Missing Skills\")\n",
        "    missing = find_missing(resume_data[\"skills\"], jd_data[\"skills\"])\n",
        "    st.write(missing)\n",
        "\n",
        "    st.subheader(\"AI Summary\")\n",
        "    summary = generate_summary(match, sim, missing, resume_data[\"skills\"], jd_data[\"skills\"])\n",
        "    st.write(summary)\n",
        "\n",
        "    st.subheader(\"Skill Match Graph\")\n",
        "    graph_path = plot_graph(resume_data[\"skills\"], jd_data[\"skills\"], missing)\n",
        "    st.image(graph_path, use_column_width=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_hXaYpa1RHX"
      },
      "outputs": [],
      "source": [
        "public_url = ngrok.connect(8501)\n",
        "public_url\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Z0xVmTIv5u3q"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py --server.port 8501"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
